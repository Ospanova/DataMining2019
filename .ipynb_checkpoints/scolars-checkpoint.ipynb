{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "try:\n",
    "    from urllib.request import HTTPCookieProcessor, Request, build_opener\n",
    "    from urllib.parse import quote, unquote\n",
    "    from http.cookiejar import MozillaCookieJar\n",
    "except ImportError:\n",
    "    from urllib2 import Request, build_opener, HTTPCookieProcessor\n",
    "    from urllib import quote, unquote\n",
    "    from cookielib import MozillaCookieJar\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError:\n",
    "    try:\n",
    "        from BeautifulSoup import BeautifulSoup\n",
    "    except ImportError:\n",
    "        print('We need BeautifulSoup, sorry...')\n",
    "        sys.exit(1)\n",
    "columns = ['title', 'url', 'year', 'keywords', 'authors', 'abstract', 'text', 'url_pdf', 'num_citations', 'url_citations', 'cluster_id', 'num_versions', 'url_versions', 'excerpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version_info[0] == 3:\n",
    "    unicode = str \n",
    "    encode = lambda s: unicode(s) \n",
    "else:\n",
    "    def encode(s):\n",
    "        if isinstance(s, basestring):\n",
    "            return s.encode('utf-8')\n",
    "        else:\n",
    "            return str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoupKitchen(object):\n",
    "    @staticmethod\n",
    "    def make_soup(markup, parser=None):\n",
    "        if 'bs4' in sys.modules:\n",
    "            if parser is None:\n",
    "                warnings.filterwarnings('ignore', 'No parser was explicitly specified')\n",
    "            return BeautifulSoup(markup, parser)\n",
    "\n",
    "        return BeautifulSoup(markup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoupKitchen(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def make_soup(markup, parser=None):\n",
    "        \n",
    "        if 'bs4' in sys.modules:\n",
    "            if parser is None:\n",
    "                warnings.filterwarnings('ignore', 'No parser was explicitly specified')\n",
    "            return BeautifulSoup(markup, parser)\n",
    "\n",
    "        return BeautifulSoup(markup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScholarConf(object):\n",
    "    VERSION = '2.10'\n",
    "    LOG_LEVEL = 1\n",
    "    MAX_PAGE_RESULTS = 1000000\n",
    "    SCHOLAR_SITE = 'http://scholar.google.com'\n",
    "    USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64; rv:27.0) Gecko/20100101 Firefox/27.0'\n",
    "    COOKIE_JAR_FILE = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScholarUtils(object):\n",
    "    LOG_LEVELS = {'error': 1,\n",
    "                  'warn':  2,\n",
    "                  'info':  3,\n",
    "                  'debug': 4}\n",
    "\n",
    "    @staticmethod\n",
    "    def ensure_int(arg, msg=None):\n",
    "        try:\n",
    "            return int(arg)\n",
    "        except ValueError:\n",
    "            raise FormatError(msg)\n",
    "\n",
    "    @staticmethod\n",
    "    def log(level, msg):\n",
    "        if level not in ScholarUtils.LOG_LEVELS.keys():\n",
    "            return\n",
    "        if ScholarUtils.LOG_LEVELS[level] > ScholarConf.LOG_LEVEL:\n",
    "            return\n",
    "        sys.stderr.write('[%5s]  %s' % (level.upper(), msg + '\\n'))\n",
    "        sys.stderr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScholarSettings(object):\n",
    "    CITFORM_NONE = 0\n",
    "    CITFORM_REFWORKS = 1\n",
    "    CITFORM_REFMAN = 2\n",
    "    CITFORM_ENDNOTE = 3\n",
    "    CITFORM_BIBTEX = 4\n",
    "\n",
    "    def __init__(self):\n",
    "        self.citform = 0\n",
    "        self.per_page_results = 400\n",
    "        self._is_configured = False\n",
    "    def is_configured(self):\n",
    "        return self._is_configured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScholarArticle(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.attrs = {\n",
    "            'title':         [None, 'Title',          0],\n",
    "            'url':           [None, 'URL',            1],\n",
    "            'year':          [None, 'Year',           2],\n",
    "            'keywords':      [None, 'Keywords',      3],\n",
    "            'authors':       [None, 'Authors',      4],\n",
    "            'abstract':      [None, 'Abstract',      5],\n",
    "            'text':          [None, 'Text', 6]\n",
    "        }\n",
    "\n",
    "        self.citation_data = None\n",
    "        \n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key in self.attrs:\n",
    "            return self.attrs[key][0]\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attrs)\n",
    "\n",
    "    def __setitem__(self, key, item):\n",
    "        if key in self.attrs:\n",
    "            self.attrs[key][0] = item\n",
    "        else:\n",
    "            self.attrs[key] = [item, key, len(self.attrs)]\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        if key in self.attrs:\n",
    "            del self.attrs[key]\n",
    "\n",
    "    def set_citation_data(self, citation_data):\n",
    "        self.citation_data = citation_data\n",
    "\n",
    "    def as_csv(self, header=False, sep='|'):\n",
    "        # Get keys sorted in specified order:\n",
    "        keys = [pair[0] for pair in \\\n",
    "                sorted([(key, val[2]) for key, val in list(self.attrs.items())],\n",
    "                       key=lambda pair: pair[1])]\n",
    "        res = []\n",
    "        if header:\n",
    "            res.append(sep.join(keys))\n",
    "        res.append(sep.join([unicode(self.attrs[key][0]) for key in keys]))\n",
    "        res = {}\n",
    "        for key in keys:\n",
    "            res[key] = self.attrs[key][0]\n",
    "        return res\n",
    "    def as_txt(self):\n",
    "        # Get items sorted in specified order:\n",
    "        items = sorted(list(self.attrs.values()), key=lambda item: item[2])\n",
    "        # Find largest label length:\n",
    "        max_label_len = max([len(str(item[1])) for item in items])\n",
    "        fmt = '%%%ds %%s' % max_label_len\n",
    "        res = []\n",
    "        for item in items:\n",
    "            if item[0] is not None:\n",
    "                res.append(fmt % (item[1], item[0]))\n",
    "        return '\\n'.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScholarArticleParser(object):\n",
    "    def __init__(self, site=None):\n",
    "        self.soup = None\n",
    "        self.article = None\n",
    "        self.site = site or ScholarConf.SCHOLAR_SITE\n",
    "        self.year_re = re.compile(r'\\b(?:20|19)\\d{2}\\b')\n",
    "        \n",
    "    def handle_article(self, art):\n",
    "        \"\"\"\n",
    "        The parser invokes this callback on each article parsed\n",
    "        successfully.  In this base class, the callback does nothing.\n",
    "        \"\"\"\n",
    "\n",
    "    def handle_num_results(self, num_results):\n",
    "        \"\"\"\n",
    "        The parser invokes this callback if it determines the overall\n",
    "        number of results, as reported on the parsed results page. The\n",
    "        base class implementation does nothing.\n",
    "        \"\"\"\n",
    "    def parse(self, html):\n",
    "\n",
    "        self.soup = SoupKitchen.make_soup(html)\n",
    "\n",
    "    \n",
    "        self._parse_globals()\n",
    "        cnt = 0;\n",
    "        for div in self.soup.findAll(ScholarArticleParser._tag_results_checker):\n",
    "            self._parse_article(div)\n",
    "            self._clean_article()\n",
    "            if self.article['title']:\n",
    "                self.handle_article(self.article)\n",
    "                cnt = cnt + 1\n",
    "            print(cnt)\n",
    "\n",
    "    def _clean_article(self):\n",
    "       \n",
    "        if self.article['title']:\n",
    "            self.article['title'] = self.article['title'].strip()\n",
    "\n",
    "    def _parse_globals(self):\n",
    "        tag = self.soup.find(name='div', attrs={'id': 'gs_ab_md'})\n",
    "        if tag is not None:\n",
    "            raw_text = tag.findAll(text=True)\n",
    "            if raw_text is not None and len(raw_text) > 0:\n",
    "                try:\n",
    "                    num_results = raw_text[0].split()[1]\n",
    "                    num_results = num_results.replace(',', '')\n",
    "                    num_results = int(num_results)\n",
    "                    self.handle_num_results(num_results)\n",
    "                except (IndexError, ValueError):\n",
    "                    pass\n",
    "\n",
    "    def _parse_article(self, div):\n",
    "        self.article = ScholarArticle()\n",
    "\n",
    "        for tag in div:\n",
    "            if not hasattr(tag, 'name'):\n",
    "                continue\n",
    "\n",
    "            if tag.name == 'div' and self._tag_has_class(tag, 'gs_rt') and \\\n",
    "                    tag.h3 and tag.h3.a:\n",
    "                self.article['title'] = ''.join(tag.h3.a.findAll(text=True))\n",
    "                self.article['url'] = self._path2url(tag.h3.a['href'])\n",
    "                if self.article['url'].endswith('.pdf'):\n",
    "                    self.article['url_pdf'] = self.article['url']\n",
    "\n",
    "            if tag.name == 'font':\n",
    "                for tag2 in tag:\n",
    "                    if not hasattr(tag2, 'name'):\n",
    "                        continue\n",
    "                    if tag2.name == 'span' and \\\n",
    "                       self._tag_has_class(tag2, 'gs_fl'):\n",
    "                        self._parse_links(tag2)\n",
    "\n",
    "    def _parse_links(self, span):\n",
    "        for tag in span:\n",
    "            if not hasattr(tag, 'name'):\n",
    "                continue\n",
    "            if tag.name != 'a' or tag.get('href') is None:\n",
    "                continue\n",
    "\n",
    "            if tag.get('href').startswith('/scholar?cites'):\n",
    "                if hasattr(tag, 'string') and tag.string.startswith('Cited by'):\n",
    "                    self.article['num_citations'] = \\\n",
    "                        self._as_int(tag.string.split()[-1])\n",
    "\n",
    "                self.article['url_citations'] = \\\n",
    "                    self._strip_url_arg('num', self._path2url(tag.get('href')))\n",
    "\n",
    "                args = self.article['url_citations'].split('?', 1)[1]\n",
    "                for arg in args.split('&'):\n",
    "                    if arg.startswith('cites='):\n",
    "                        self.article['cluster_id'] = arg[6:]\n",
    "\n",
    "            if tag.get('href').startswith('/scholar?cluster'):\n",
    "                if hasattr(tag, 'string') and tag.string.startswith('All '):\n",
    "                    self.article['num_versions'] = \\\n",
    "                        self._as_int(tag.string.split()[1])\n",
    "                self.article['url_versions'] = \\\n",
    "                    self._strip_url_arg('num', self._path2url(tag.get('href')))\n",
    "\n",
    "            if tag.getText().startswith('Import'):\n",
    "                self.article['url_citation'] = self._path2url(tag.get('href'))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _tag_has_class(tag, klass):\n",
    "    \n",
    "        res = tag.get('class') or []\n",
    "        if type(res) != list:\n",
    "            res = res.split()\n",
    "        return klass in res\n",
    "\n",
    "    @staticmethod\n",
    "    def _tag_results_checker(tag):\n",
    "        return tag.name == 'div' \\\n",
    "            and ScholarArticleParser._tag_has_class(tag, 'gs_r')\n",
    "\n",
    "    @staticmethod\n",
    "    def _as_int(obj):\n",
    "        try:\n",
    "            return int(obj)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def _path2url(self, path):\n",
    "        \"\"\"Helper, returns full URL in case path isn't one.\"\"\"\n",
    "        if path.startswith('http://'):\n",
    "            return path\n",
    "        if not path.startswith('/'):\n",
    "            path = '/' + path\n",
    "        return self.site + path\n",
    "\n",
    "    def _strip_url_arg(self, arg, url):\n",
    "        \"\"\"Helper, removes a URL-encoded argument, if present.\"\"\"\n",
    "        parts = url.split('?', 1)\n",
    "        if len(parts) != 2:\n",
    "            return url\n",
    "        res = []\n",
    "        for part in parts[1].split('&'):\n",
    "            if not part.startswith(arg + '='):\n",
    "                res.append(part)\n",
    "        return parts[0] + '?' + '&'.join(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScholarArticleParser120726(ScholarArticleParser):\n",
    "    def _parse_article(self, div):\n",
    "        self.article = ScholarArticle()\n",
    "\n",
    "        for tag in div:\n",
    "            if not hasattr(tag, 'name'):\n",
    "                continue\n",
    "            if str(tag).lower().find('.pdf'):\n",
    "                if tag.find('div', {'class': 'gs_ttss'}):\n",
    "                    self._parse_links(tag.find('div', {'class': 'gs_ttss'}))\n",
    "\n",
    "            if tag.name == 'div' and self._tag_has_class(tag, 'gs_ri'):\n",
    "                try:\n",
    "                    atag = tag.h3.a\n",
    "                    self.article['title'] = ''.join(atag.findAll(text=True))\n",
    "                    self.article['url'] = self._path2url(atag['href'])\n",
    "                    if self.article['url'].endswith('.pdf'):\n",
    "                        self.article['url_pdf'] = self.article['url']\n",
    "                except:\n",
    "                    # Remove a few spans that have unneeded content (e.g. [CITATION])\n",
    "                    for span in tag.h3.findAll(name='span'):\n",
    "                        span.clear()\n",
    "                    self.article['title'] = ''.join(tag.h3.findAll(text=True))\n",
    "\n",
    "                if tag.find('div', {'class': 'gs_a'}):\n",
    "                    year = self.year_re.findall(tag.find('div', {'class': 'gs_a'}).text)\n",
    "                    self.article['year'] = year[0] if len(year) > 0 else None\n",
    "\n",
    "                if tag.find('div', {'class': 'gs_fl'}):\n",
    "                    self._parse_links(tag.find('div', {'class': 'gs_fl'}))\n",
    "\n",
    "                if tag.find('div', {'class': 'gs_rs'}):\n",
    "                    # These are the content excerpts rendered into the results.\n",
    "                    raw_text = tag.find('div', {'class': 'gs_rs'}).findAll(text=True)\n",
    "                    if len(raw_text) > 0:\n",
    "                        raw_text = ''.join(raw_text)\n",
    "                        raw_text = raw_text.replace('\\n', '')\n",
    "                        self.article['excerpt'] = raw_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScholarQuerier(object):\n",
    "   \n",
    "    GET_SETTINGS_URL = ScholarConf.SCHOLAR_SITE + '/scholar_settings?' \\\n",
    "        + 'sciifh=2&hl=en&as_sdt=0,5'\n",
    "\n",
    "    SET_SETTINGS_URL = ScholarConf.SCHOLAR_SITE + '/scholar_setprefs?' \\\n",
    "        + 'q=' \\\n",
    "        + '&scisig=%(scisig)s' \\\n",
    "        + '&inststart=0' \\\n",
    "        + '&as_sdt=1,5' \\\n",
    "        + '&as_sdtp=' \\\n",
    "        + '&num=%(num)s' \\\n",
    "        + '&scis=%(scis)s' \\\n",
    "        + '%(scisf)s' \\\n",
    "        + '&hl=en&lang=all&instq=&inst=569367360547434339&save='\n",
    "\n",
    "    class Parser(ScholarArticleParser120726):\n",
    "        def __init__(self, querier):\n",
    "            ScholarArticleParser120726.__init__(self)\n",
    "            self.querier = querier\n",
    "\n",
    "        def handle_num_results(self, num_results):\n",
    "            print(\"NUMS\")\n",
    "            print(num_results)\n",
    "            if self.querier is not None and self.querier.query is not None:\n",
    "                self.querier.query['num_results'] = num_results\n",
    "\n",
    "        def handle_article(self, art):\n",
    "            self.querier.add_article(art)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.articles = []\n",
    "        self.query = None\n",
    "        self.cjar = MozillaCookieJar()\n",
    "\n",
    "        if ScholarConf.COOKIE_JAR_FILE and \\\n",
    "           os.path.exists(ScholarConf.COOKIE_JAR_FILE):\n",
    "            try:\n",
    "                self.cjar.load(ScholarConf.COOKIE_JAR_FILE,\n",
    "                               ignore_discard=True)\n",
    "                ScholarUtils.log('info', 'loaded cookies file')\n",
    "            except Exception as msg:\n",
    "                ScholarUtils.log('warn', 'could not load cookies file: %s' % msg)\n",
    "                self.cjar = MozillaCookieJar() \n",
    "        self.opener = build_opener(HTTPCookieProcessor(self.cjar))\n",
    "        self.settings = None \n",
    "    def apply_settings(self, settings):\n",
    "        \"\"\"\n",
    "        Applies settings as provided by a ScholarSettings instance.\n",
    "        \"\"\"\n",
    "#         if settings is None or not settings.is_configured():\n",
    "#             return True\n",
    "\n",
    "        self.settings = settings\n",
    "\n",
    "        \n",
    "        html = self._get_http_response(url=self.GET_SETTINGS_URL,\n",
    "                                       log_msg='dump of settings form HTML',\n",
    "                                       err_msg='requesting settings failed')\n",
    "        if html is None:\n",
    "            return False\n",
    "\n",
    "       \n",
    "        soup = SoupKitchen.make_soup(html)\n",
    "\n",
    "        tag = soup.find(name='form', attrs={'id': 'gs_settings_form'})\n",
    "        if tag is None:\n",
    "            ScholarUtils.log('info', 'parsing settings failed: no form')\n",
    "            return False\n",
    "\n",
    "        tag = tag.find('input', attrs={'type':'hidden', 'name':'scisig'})\n",
    "        if tag is None:\n",
    "            ScholarUtils.log('info', 'parsing settings failed: scisig')\n",
    "            return False\n",
    "\n",
    "        urlargs = {'scisig': tag['value'],\n",
    "                   'num': settings.per_page_results,\n",
    "                   'scis': 'no',\n",
    "                   'scisf': ''}\n",
    "\n",
    "        if settings.citform != 0:\n",
    "            urlargs['scis'] = 'yes'\n",
    "            urlargs['scisf'] = '&scisf=%d' % settings.citform\n",
    "\n",
    "        html = self._get_http_response(url=self.SET_SETTINGS_URL % urlargs,\n",
    "                                       log_msg='dump of settings result HTML',\n",
    "                                       err_msg='applying setttings failed')\n",
    "        if html is None:\n",
    "            return False\n",
    "\n",
    "        ScholarUtils.log('info', 'settings applied')\n",
    "        return True\n",
    "\n",
    "    def send_query(self, query):\n",
    "        self.clear_articles()\n",
    "        self.query = query\n",
    "        print('URL\\n')\n",
    "        print(query.get_url())\n",
    "        html = self._get_http_response(url=query.get_url(),\n",
    "                                       log_msg='dump of query response HTML',\n",
    "                                       err_msg='results retrieval failed')\n",
    "        if html is None:\n",
    "            return\n",
    "\n",
    "        self.parse(html)\n",
    "\n",
    "    def get_citation_data(self, article):\n",
    "        if article['url_citation'] is None:\n",
    "            return False\n",
    "        if article.citation_data is not None:\n",
    "            return True\n",
    "\n",
    "        ScholarUtils.log('info', 'retrieving citation export data')\n",
    "        data = self._get_http_response(url=article['url_citation'],\n",
    "                                       log_msg='citation data response',\n",
    "                                       err_msg='requesting citation data failed')\n",
    "        if data is None:\n",
    "            return False\n",
    "\n",
    "        article.set_citation_data(data)\n",
    "        return True\n",
    "\n",
    "    def parse(self, html):\n",
    "        parser = self.Parser(self)\n",
    "        parser.parse(html)\n",
    "\n",
    "    def add_article(self, art):\n",
    "        self.get_citation_data(art)\n",
    "        self.articles.append(art)\n",
    "\n",
    "    def clear_articles(self):\n",
    "        self.articles = []\n",
    "\n",
    "    def save_cookies(self):\n",
    "        if ScholarConf.COOKIE_JAR_FILE is None:\n",
    "            return False\n",
    "        try:\n",
    "            self.cjar.save(ScholarConf.COOKIE_JAR_FILE,\n",
    "                           ignore_discard=True)\n",
    "            ScholarUtils.log('info', 'saved cookies file')\n",
    "            return True\n",
    "        except Exception as msg:\n",
    "            ScholarUtils.log('warn', 'could not save cookies file: %s' % msg)\n",
    "            return False\n",
    "\n",
    "    def _get_http_response(self, url, log_msg=None, err_msg=None):\n",
    "       \n",
    "        if log_msg is None:\n",
    "            log_msg = 'HTTP response data follow'\n",
    "        if err_msg is None:\n",
    "            err_msg = 'request failed'\n",
    "        try:\n",
    "            ScholarUtils.log('info', 'requesting %s' % unquote(url))\n",
    "\n",
    "            req = Request(url=url, headers={'User-Agent': ScholarConf.USER_AGENT})\n",
    "            hdl = self.opener.open(req)\n",
    "            html = hdl.read()\n",
    "\n",
    "            ScholarUtils.log('debug', log_msg)\n",
    "            ScholarUtils.log('debug', '>>>>' + '-'*68)\n",
    "            ScholarUtils.log('debug', 'url: %s' % hdl.geturl())\n",
    "            ScholarUtils.log('debug', 'result: %s' % hdl.getcode())\n",
    "            ScholarUtils.log('debug', 'headers:\\n' + str(hdl.info()))\n",
    "            ScholarUtils.log('debug', 'data:\\n' + html.decode('utf-8')) # For Python 3\n",
    "            ScholarUtils.log('debug', '<<<<' + '-'*68)\n",
    "\n",
    "            return html\n",
    "        except Exception as err:\n",
    "            ScholarUtils.log('info', err_msg + ': %s' % err)\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScholarQuery(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.url = None\n",
    "        self.num_results = None\n",
    "        self.attrs = {}\n",
    "\n",
    "    def set_num_page_results(self, num_page_results):\n",
    "        self.num_results = ScholarUtils.ensure_int(\n",
    "            num_page_results,\n",
    "            'maximum number of results on page must be numeric')\n",
    "\n",
    "    def get_url(self):\n",
    "        return None\n",
    "\n",
    "    def _add_attribute_type(self, key, label, default_value=None):\n",
    "        if len(self.attrs) == 0:\n",
    "            self.attrs[key] = [default_value, label, 0]\n",
    "            return\n",
    "        idx = max([item[2] for item in self.attrs.values()]) + 1\n",
    "        self.attrs[key] = [default_value, label, idx]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key in self.attrs:\n",
    "            return self.attrs[key][0]\n",
    "        return None\n",
    "\n",
    "    def __setitem__(self, key, item):\n",
    "        for key in self.attrs:\n",
    "            self.attrs[key][0] = item\n",
    "\n",
    "    def _parenthesize_phrases(self, query):\n",
    "        if query.find(',') < 0:\n",
    "            return query\n",
    "        phrases = []\n",
    "        for phrase in query.split(','):\n",
    "            phrase = phrase.strip()\n",
    "            if phrase.find(' ') > 0:\n",
    "                phrase = '\"' + phrase + '\"'\n",
    "            phrases.append(phrase)\n",
    "        return ' '.join(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchScholarQuery(ScholarQuery):\n",
    "    \"\"\"\n",
    "    This version represents the search query parameters the user can\n",
    "    configure on the Scholar website, in the advanced search options.\n",
    "    \"\"\"\n",
    "    SCHOLAR_QUERY_URL = ScholarConf.SCHOLAR_SITE + '/scholar?' \\\n",
    "        + 'as_q=%(words)s' \\\n",
    "        + '&as_epq=%(phrase)s' \\\n",
    "        + '&as_oq=%(words_some)s' \\\n",
    "        + '&as_eq=%(words_none)s' \\\n",
    "        + '&as_occt=%(scope)s' \\\n",
    "        + '&as_sauthors=%(authors)s' \\\n",
    "        + '&as_publication=%(pub)s' \\\n",
    "        + '&as_ylo=%(ylo)s' \\\n",
    "        + '&as_yhi=%(yhi)s' \\\n",
    "        + '&as_vis=%(citations)s' \\\n",
    "        + '&btnG=&hl=en' \\\n",
    "        + '%(num)s' \\\n",
    "        + '&as_sdt=%(patents)s%%2C5'\n",
    "\n",
    "    def __init__(self):\n",
    "        ScholarQuery.__init__(self)\n",
    "        self._add_attribute_type('num_results', 'Results', 0)\n",
    "        self.words = None # The default search behavior\n",
    "        self.words_some = None # At least one of those words\n",
    "        self.words_none = None # None of these words\n",
    "        self.phrase = None\n",
    "        self.scope_title = False # If True, search in title only\n",
    "        self.author = None\n",
    "        self.pub = None\n",
    "        self.timeframe = [None, None]\n",
    "        self.include_patents = True\n",
    "        self.include_citations = True\n",
    "\n",
    "    def set_words(self, words):\n",
    "        \"\"\"Sets words that *all* must be found in the result.\"\"\"\n",
    "        self.words = words\n",
    "\n",
    "    def set_words_some(self, words):\n",
    "        \"\"\"Sets words of which *at least one* must be found in result.\"\"\"\n",
    "        self.words_some = words\n",
    "\n",
    "    def set_words_none(self, words):\n",
    "        \"\"\"Sets words of which *none* must be found in the result.\"\"\"\n",
    "        self.words_none = words\n",
    "\n",
    "    def set_phrase(self, phrase):\n",
    "        \"\"\"Sets phrase that must be found in the result exactly.\"\"\"\n",
    "        self.phrase = phrase\n",
    "\n",
    "    def set_scope(self, title_only):\n",
    "        \"\"\"\n",
    "        Sets Boolean indicating whether to search entire article or title\n",
    "        only.\n",
    "        \"\"\"\n",
    "        self.scope_title = title_only\n",
    "\n",
    "    def set_author(self, author):\n",
    "        \"\"\"Sets names that must be on the result's author list.\"\"\"\n",
    "        self.author = author\n",
    "\n",
    "    def set_pub(self, pub):\n",
    "        \"\"\"Sets the publication in which the result must be found.\"\"\"\n",
    "        self.pub = pub\n",
    "\n",
    "    def set_timeframe(self, start=None, end=None):\n",
    "        \"\"\"\n",
    "        Sets timeframe (in years as integer) in which result must have\n",
    "        appeared. It's fine to specify just start or end, or both.\n",
    "        \"\"\"\n",
    "        if start:\n",
    "            start = ScholarUtils.ensure_int(start)\n",
    "        if end:\n",
    "            end = ScholarUtils.ensure_int(end)\n",
    "        self.timeframe = [start, end]\n",
    "\n",
    "    def set_include_citations(self, yesorno):\n",
    "        self.include_citations = yesorno\n",
    "\n",
    "    def set_include_patents(self, yesorno):\n",
    "        self.include_patents = yesorno\n",
    "\n",
    "    def get_url(self):\n",
    "        if self.words is None and self.words_some is None \\\n",
    "           and self.words_none is None and self.phrase is None \\\n",
    "           and self.author is None and self.pub is None \\\n",
    "           and self.timeframe[0] is None and self.timeframe[1] is None:\n",
    "            raise QueryArgumentError('search query needs more parameters')\n",
    "\n",
    "        # If we have some-words or none-words lists, we need to\n",
    "        # process them so GS understands them. For simple\n",
    "        # space-separeted word lists, there's nothing to do. For lists\n",
    "        # of phrases we have to ensure quotations around the phrases,\n",
    "        # separating them by whitespace.\n",
    "        words_some = None\n",
    "        words_none = None\n",
    "\n",
    "        if self.words_some:\n",
    "            words_some = self._parenthesize_phrases(self.words_some)\n",
    "        if self.words_none:\n",
    "            words_none = self._parenthesize_phrases(self.words_none)\n",
    "\n",
    "        urlargs = {'words': self.words or '',\n",
    "                   'words_some': words_some or '',\n",
    "                   'words_none': words_none or '',\n",
    "                   'phrase': self.phrase or '',\n",
    "                   'scope': 'title' if self.scope_title else 'any',\n",
    "                   'authors': self.author or '',\n",
    "                   'pub': self.pub or '',\n",
    "                   'ylo': self.timeframe[0] or '',\n",
    "                   'yhi': self.timeframe[1] or '',\n",
    "                   'patents': '0' if self.include_patents else '1',\n",
    "                   'citations': '0' if self.include_citations else '1'}\n",
    "\n",
    "        for key, val in urlargs.items():\n",
    "            urlargs[key] = quote(encode(val))\n",
    "\n",
    "        # The following URL arguments must not be quoted, or the\n",
    "        # server will not recognize them:\n",
    "        urlargs['num'] = ('&num=%d' % self.num_results\n",
    "                          if self.num_results is not None else '')\n",
    "\n",
    "        return self.SCHOLAR_QUERY_URL % urlargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvv(querier, header=False, sep='|'):\n",
    "    max_label_len = 0\n",
    "    if len(querier.articles) > 0:\n",
    "        items = sorted(list(querier.articles[0].attrs.values()),\n",
    "                       key=lambda item: item[2])\n",
    "        print(items)\n",
    "        max_label_len = max([len(str(item[1])) for item in items])\n",
    "\n",
    "        # Get items sorted in specified order:\n",
    "        items = sorted(list(querier.query.attrs.values()), key=lambda item: item[2])\n",
    "        # Find largest label length:\n",
    "        max_label_len = max([len(str(item[1])) for item in items] + [max_label_len])\n",
    "        fmt = '[G] %%%ds %%s' % max(0, max_label_len-4)\n",
    "        for item in items:\n",
    "            if item[0] is not None:\n",
    "                print(fmt % (item[1], item[0]))\n",
    "        if len(items) > 0:\n",
    "            print\n",
    "    articles = querier.articles\n",
    "    with open('result.csv', mode='a') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        for art in articles:\n",
    "            result = art.as_csv(header=header, sep=sep)\n",
    "            writer.writerow(result)\n",
    "            header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL\n",
      "\n",
      "http://scholar.google.com/scholar?as_q=edge%20counting&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=&as_publication=&as_ylo=&as_yhi=&as_vis=0&btnG=&hl=en&num=2350000&as_sdt=0%2C5\n",
      "NUMS\n",
      "2360000\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "20\n",
      "SIZE\n",
      "20\n",
      "[['A new similarity measure based on edge counting', 'Title', 0], ['http://scholar.google.com/https://pdfs.semanticscholar.org/02a5/6176a48ba9de3a6e43e7e10da531ee4adfb8.pdf', 'URL', 1], ['2006', 'Year', 2], [None, 'Keywords', 3], [None, 'Authors', 4], [None, 'Abstract', 5], [None, 'Text', 6], ['http://scholar.google.com/https://pdfs.semanticscholar.org/02a5/6176a48ba9de3a6e43e7e10da531ee4adfb8.pdf', 'url_pdf', 7], [63, 'num_citations', 8], ['http://scholar.google.com/scholar?cites=17316492695987697519&as_sdt=2005&sciodt=0,5&hl=en', 'url_citations', 9], ['17316492695987697519', 'cluster_id', 10], [5, 'num_versions', 11], ['http://scholar.google.com/scholar?cluster=17316492695987697519&hl=en&as_sdt=0,5', 'url_versions', 12], ['Palmer [1] has the advantage of being simple to implement and have good performances compared to the other similarity measures [2]. Nevertheless, the Wu and Palmer measure present the following disadvantage: in some situations, the similarity of two elements of an IS\\xa0…', 'excerpt', 13]]\n",
      "[G]   Results 2360000\n",
      "20\n",
      "DICtIONARY\n",
      "{'title': 'A new similarity measure based on edge counting', 'url': 'http://scholar.google.com/https://pdfs.semanticscholar.org/02a5/6176a48ba9de3a6e43e7e10da531ee4adfb8.pdf', 'year': '2006', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'url_pdf': 'http://scholar.google.com/https://pdfs.semanticscholar.org/02a5/6176a48ba9de3a6e43e7e10da531ee4adfb8.pdf', 'num_citations': 63, 'url_citations': 'http://scholar.google.com/scholar?cites=17316492695987697519&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '17316492695987697519', 'num_versions': 5, 'url_versions': 'http://scholar.google.com/scholar?cluster=17316492695987697519&hl=en&as_sdt=0,5', 'excerpt': 'Palmer [1] has the advantage of being simple to implement and have good performances compared to the other similarity measures [2]. Nevertheless, the Wu and Palmer measure present the following disadvantage: in some situations, the similarity of two elements of an IS\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'A WordNet-based semantic similarity measurement combining edge-counting and information content theory', 'url': 'http://scholar.google.com/https://www.sciencedirect.com/science/article/pii/S0952197614002814', 'year': '2015', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 70, 'url_citations': 'http://scholar.google.com/scholar?cites=9203904684700800121&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '9203904684700800121', 'num_versions': 2, 'url_versions': 'http://scholar.google.com/scholar?cluster=9203904684700800121&hl=en&as_sdt=0,5', 'excerpt': 'Semantic similarity measuring between words can be applied to many applications, such as Artificial Intelligence, Information Processing, Medical Care and Linguistics. In this paper, we present a new approach for semantic similarity measuring which is based on edge-counting \\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Learning and verifying graphs using queries with a focus on edge counting', 'url': 'http://scholar.google.com/https://link.springer.com/chapter/10.1007/978-3-540-75225-7_24', 'year': '2007', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 39, 'url_citations': 'http://scholar.google.com/scholar?cites=8780693494039253091&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '8780693494039253091', 'num_versions': 13, 'url_versions': 'http://scholar.google.com/scholar?cluster=8780693494039253091&hl=en&as_sdt=0,5', 'excerpt': 'We consider the problem of learning and verifying hidden graphs and their properties given query access to the graphs. We analyze various queries (edge detection, edge counting, shortest path), but we focus mainly on edge counting queries. We give an algorithm for\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Detection and tracking of facial features by using edge pixel counting and deformable circular template matching', 'url': 'http://scholar.google.com/https://search.ieice.org/bin/summary.php?id=e78-d_9_1195', 'year': '1995', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 34, 'url_citations': 'http://scholar.google.com/scholar?cites=7209571638465748175&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '7209571638465748175', 'num_versions': 4, 'url_versions': 'http://scholar.google.com/scholar?cluster=7209571638465748175&hl=en&as_sdt=0,5', 'excerpt': 'In this paper face feature detection and tracking are discussed, using methods called edge pixel counting and deformable circular template matching. Instead of utilizing color or gray scale information of the facial image, the proposed edge pixel counting method utilizes the\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'A new similarity measure for taxonomy based on edge counting', 'url': 'http://scholar.google.com/https://arxiv.org/abs/1211.4709', 'year': '2012', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 22, 'url_citations': 'http://scholar.google.com/scholar?cites=1114639821764738177&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '1114639821764738177', 'num_versions': 4, 'url_versions': 'http://scholar.google.com/scholar?cluster=1114639821764738177&hl=en&as_sdt=0,5', 'excerpt': 'This paper introduces a new similarity measure based on edge counting in a taxonomy like WorldNet or Ontology. Measurement of similarity between text segments or concepts is very useful for many applications like information retrieval, ontology matching, text mining, and\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Experimental feasibility of multi-energy photon-counting K-edge imaging in pre-clinical computed tomography', 'url': 'http://scholar.google.com/https://iopscience.iop.org/article/10.1088/0031-9155/53/15/002', 'year': '2008', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 701, 'url_citations': 'http://scholar.google.com/scholar?cites=167065703869004417&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '167065703869004417', 'num_versions': 10, 'url_versions': 'http://scholar.google.com/scholar?cluster=167065703869004417&hl=en&as_sdt=0,5', 'excerpt': 'Theoretical considerations predicted the feasibility of K-edge x-ray computed tomography (CT) imaging using energy discriminating detectors with more than two energy bins. This technique enables material-specific imaging in CT, which in combination with high-Z\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'K-edge imaging in x-ray computed tomography using multi-bin photon counting detectors', 'url': 'http://scholar.google.com/https://iopscience.iop.org/article/10.1088/0031-9155/52/15/020/pdf', 'year': '2007', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 486, 'url_citations': 'http://scholar.google.com/scholar?cites=7568229710689612534&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '7568229710689612534', 'num_versions': 7, 'url_versions': 'http://scholar.google.com/scholar?cluster=7568229710689612534&hl=en&as_sdt=0,5', 'excerpt': 'After passage through matter, the energy spectrum of a polychromatic beam of x-rays contains valuable information about the elemental composition of the absorber. Conventional x-ray systems or x-ray computed tomography (CT) systems, equipped with\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Edge-counting vectors, Fibonacci cubes, and Fibonacci triangle', 'url': 'http://scholar.google.com/https://www.fmf.uni-lj.si/~klavzar/preprints/projections-final.pdf', 'year': '2007', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'url_pdf': 'http://scholar.google.com/https://www.fmf.uni-lj.si/~klavzar/preprints/projections-final.pdf', 'num_citations': 14, 'url_citations': 'http://scholar.google.com/scholar?cites=16200613429338166669&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '16200613429338166669', 'num_versions': 6, 'url_versions': 'http://scholar.google.com/scholar?cluster=16200613429338166669&hl=en&as_sdt=0,5', 'excerpt': 'Edge-counting vectors of subgraphs of Cartesian products are introduced as the counting vectors of the edges that project onto the factors. For several standard constructions their edge-counting vectors are computed. It is proved that the edge-counting vectors of Fibonacci\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Multienergy Photon-counting K-edge Imaging: Potential for Improved Luminal Depiction in Vascular Imaging1', 'url': 'http://scholar.google.com/https://pubs.rsna.org/doi/abs/10.1148/radiol.2492080560', 'year': '2008', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 193, 'url_citations': 'http://scholar.google.com/scholar?cites=5535654396538378281&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '5535654396538378281', 'num_versions': 3, 'url_versions': 'http://scholar.google.com/scholar?cluster=5535654396538378281&hl=en&as_sdt=0,5', 'excerpt': 'The purpose of this study was to investigate whether spectral computed tomography (CT) has the potential to improve luminal depiction by differentiating among intravascular gadolinium-based contrast agent, calcified plaque, and stent material by using the\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Measuring semantic similarity based on weighting attributes of edge counting', 'url': 'http://scholar.google.com/https://link.springer.com/chapter/10.1007/978-3-540-30583-5_50', 'year': '2004', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 14, 'url_citations': 'http://scholar.google.com/scholar?cites=16259608245888782378&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '16259608245888782378', 'num_versions': 8, 'url_versions': 'http://scholar.google.com/scholar?cluster=16259608245888782378&hl=en&as_sdt=0,5', 'excerpt': 'Semantic similarity measurement can be applied in many different fields and has variety of ways to measure it. As a foundation paper for semantic similarity, we explored the edge counting method for measuring semantic similarity by considering the weighting attributes\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Sensitivity of Photon-Counting Based-Edge Imaging in X-ray Computed Tomography', 'url': 'http://scholar.google.com/https://ieeexplore.ieee.org/abstract/document/5749698/', 'year': '2011', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 93, 'url_citations': 'http://scholar.google.com/scholar?cites=1023004115160440315&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '1023004115160440315', 'num_versions': 5, 'url_versions': 'http://scholar.google.com/scholar?cluster=1023004115160440315&hl=en&as_sdt=0,5', 'excerpt': 'The feasibility of K-edge imaging using energy-resolved, photon-counting transmission measurements in X-ray computed tomography (CT) has been demonstrated by simulations and experiments. The method is based on probing the discontinuities of the attenuation\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'A viewpoint invariant approach for crowd counting', 'url': 'http://scholar.google.com/https://ieeexplore.ieee.org/abstract/document/1699738/', 'year': '2006', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 271, 'url_citations': 'http://scholar.google.com/scholar?cites=6318337892306533388&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '6318337892306533388', 'num_versions': 10, 'url_versions': 'http://scholar.google.com/scholar?cluster=6318337892306533388&hl=en&as_sdt=0,5', 'excerpt': '… The counting is done offline so that we can compare the result with ground truth. The estimationresults using linear fitting and neural network are shown in figure 3(a)(b). For comparison, wealso show the counting results by simply using the edge length and total blob pixels\\xa0… '}\n",
      "DICtIONARY\n",
      "{'title': 'Similarity measure based on edge counting using ontology', 'url': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.299.3500&rep=rep1&type=pdf', 'year': '2012', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 7, 'url_citations': 'http://scholar.google.com/scholar?cites=9315044924023669088&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '9315044924023669088', 'num_versions': 3, 'url_versions': 'http://scholar.google.com/scholar?cluster=9315044924023669088&hl=en&as_sdt=0,5', 'excerpt': 'Building the ontology from the scratch is a difficult process and there is no proper fully automated ontology construction methodology is available. But more and more ontologies are created and available in the web, reusing the existing ontology is reasonable for the\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Photon counting spectral CT: improved material decomposition with K-edge-filtered x-rays', 'url': 'http://scholar.google.com/https://iopscience.iop.org/article/10.1088/0031-9155/57/6/1595/meta', 'year': '2012', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 69, 'url_citations': 'http://scholar.google.com/scholar?cites=1918201217326891947&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '1918201217326891947', 'num_versions': 6, 'url_versions': 'http://scholar.google.com/scholar?cluster=1918201217326891947&hl=en&as_sdt=0,5', 'excerpt': 'Photon counting spectral computed tomography (PCSCT) provides material selective CT imaging at a single CT scan and fixed tube voltage. The PCSCT data are acquired in several energy ranges (bins) arranged over the x-ray spectrum. The quasi-monoenergetic CT\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Radiative and nonradiative lifetimes of band edge states and deep trap states of CdS nanoparticles determined by time-correlated single photon counting', 'url': 'http://scholar.google.com/https://www.sciencedirect.com/science/article/pii/S0009261400011143', 'year': '2000', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 101, 'url_citations': 'http://scholar.google.com/scholar?cites=11882018954035962770&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '11882018954035962770', 'num_versions': 6, 'url_versions': 'http://scholar.google.com/scholar?cluster=11882018954035962770&hl=en&as_sdt=0,5', 'excerpt': 'Emission lifetimes of band edge and deep trap states of CdS nanoparticles with different surface capping were measured. For unpassivated nanoparticles with low fluorescence, the emission is dominated by deep trap states and the decay (5 ns) is independent of excitation\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Design-based stereological methods for counting neurons', 'url': 'http://scholar.google.com/https://www.sciencedirect.com/science/article/pii/S0079612302350064', 'year': '2002', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 117, 'url_citations': 'http://scholar.google.com/scholar?cites=17332014435390851558&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '17332014435390851558', 'num_versions': 6, 'url_versions': 'http://scholar.google.com/scholar?cluster=17332014435390851558&hl=en&as_sdt=0,5', 'excerpt': '… entirety. Using the leading edge counting rule, three objects are counted in the redsection. (D\\xa0… section. An object is counted if its leading edge comes into focus within thecounting frame, as the latter is moved through the section. The\\xa0… '}\n",
      "DICtIONARY\n",
      "{'title': 'A simple FPTAS for counting edge covers', 'url': 'http://scholar.google.com/https://epubs.siam.org/doi/abs/10.1137/1.9781611973402.25', 'year': '2014', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 32, 'url_citations': 'http://scholar.google.com/scholar?cites=7257130669366347978&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '7257130669366347978', 'num_versions': 8, 'url_versions': 'http://scholar.google.com/scholar?cluster=7257130669366347978&hl=en&as_sdt=0,5', 'excerpt': 'An edge cover of a graph is a set of edges such that every vertex has at least an adjacent edge in it. We design a very simple deterministic fully polynomial-time approximation scheme (FPTAS) for counting the number of edge covers for any graph. Previously\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Edge-responsive apparatus for counting conveyor-transported articles', 'url': 'http://scholar.google.com/https://patents.google.com/patent/US4384195A/en', 'year': '1983', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 35, 'url_citations': 'http://scholar.google.com/scholar?cites=8036024126360401796&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '8036024126360401796', 'num_versions': 2, 'url_versions': 'http://scholar.google.com/scholar?cluster=8036024126360401796&hl=en&as_sdt=0,5', 'excerpt': 'Apparatus for counting conveyor-transported articles, such as newspapers. The apparatus employes a laser which projects a beam to create reflections from such articles, which reflections are imaged onto a linear photodetector array. The array is scanned recurrently to\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Spectral photon-counting CT: Initial experience with dual–contrast agent K-edge colonography', 'url': 'http://scholar.google.com/https://pubs.rsna.org/doi/abs/10.1148/radiol.2016160890', 'year': '2016', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 51, 'url_citations': 'http://scholar.google.com/scholar?cites=8058887870384790841&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '8058887870384790841', 'num_versions': 6, 'url_versions': 'http://scholar.google.com/scholar?cluster=8058887870384790841&hl=en&as_sdt=0,5', 'excerpt': 'Purpose To investigate the feasibility of using spectral photon-counting computed tomography (CT) to differentiate between gadolinium-based and nonionic iodine-based contrast material in a colon phantom by using the characteristic k edge of gadolinium\\xa0…'}\n",
      "DICtIONARY\n",
      "{'title': 'Notes on the estimation of the numerical density of arbitrary profiles: the edge effect', 'url': 'http://scholar.google.com/https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2818.1977.tb00062.x', 'year': '1977', 'keywords': None, 'authors': None, 'abstract': None, 'text': None, 'num_citations': 1417, 'url_citations': 'http://scholar.google.com/scholar?cites=15554166554103931969&as_sdt=2005&sciodt=0,5&hl=en', 'cluster_id': '15554166554103931969', 'num_versions': 3, 'url_versions': 'http://scholar.google.com/scholar?cluster=15554166554103931969&hl=en&as_sdt=0,5', 'excerpt': '… 2A). Employing rule A we count half of the circles. Circles have well-defined centre-points andby counting these zero-dimensional structures instead of the circles we avoid the edge effect,and obtain an unbiased estimate of the number 15* 219 Page 2\\xa0… '}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "querier = ScholarQuerier()\n",
    "settings = ScholarSettings()\n",
    "\n",
    "querier.apply_settings(settings)\n",
    "\n",
    "\n",
    "query = SearchScholarQuery()\n",
    "search = \"edge counting\"\n",
    "query.set_words(search)\n",
    "query.set_num_page_results(2350000)\n",
    "querier.send_query(query)\n",
    "\n",
    "\n",
    "csvv(querier)\n",
    "#txt(querier, with_globals=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
